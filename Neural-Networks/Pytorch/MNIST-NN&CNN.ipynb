{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c97e87c",
   "metadata": {},
   "source": [
    "# Using pytorch to do MINST once using NN and once using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4212fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e321e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61015248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Device configuration (MPS FOR MACBOOK)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.xticks([])  # Remove x-axis tick labels\n",
    "    plt.yticks([])  # Remove y-axis tick labels\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e59fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.4634\n",
      "Epoch [1/10], Step [200/600], Loss: 0.2579\n",
      "Epoch [1/10], Step [300/600], Loss: 0.1575\n",
      "Epoch [1/10], Step [400/600], Loss: 0.1813\n",
      "Epoch [1/10], Step [500/600], Loss: 0.1774\n",
      "Epoch [1/10], Step [600/600], Loss: 0.2076\n",
      "Epoch [2/10], Step [100/600], Loss: 0.0687\n",
      "Epoch [2/10], Step [200/600], Loss: 0.1547\n",
      "Epoch [2/10], Step [300/600], Loss: 0.1326\n",
      "Epoch [2/10], Step [400/600], Loss: 0.0860\n",
      "Epoch [2/10], Step [500/600], Loss: 0.1096\n",
      "Epoch [2/10], Step [600/600], Loss: 0.1763\n",
      "Epoch [3/10], Step [100/600], Loss: 0.1232\n",
      "Epoch [3/10], Step [200/600], Loss: 0.0407\n",
      "Epoch [3/10], Step [300/600], Loss: 0.0581\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0325\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0420\n",
      "Epoch [3/10], Step [600/600], Loss: 0.0718\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0330\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0208\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0394\n",
      "Epoch [4/10], Step [400/600], Loss: 0.0219\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0270\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0960\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0168\n",
      "Epoch [5/10], Step [200/600], Loss: 0.0344\n",
      "Epoch [5/10], Step [300/600], Loss: 0.0134\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0447\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0261\n",
      "Epoch [5/10], Step [600/600], Loss: 0.0467\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0311\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0175\n",
      "Epoch [6/10], Step [300/600], Loss: 0.0298\n",
      "Epoch [6/10], Step [400/600], Loss: 0.0369\n",
      "Epoch [6/10], Step [500/600], Loss: 0.0141\n",
      "Epoch [6/10], Step [600/600], Loss: 0.0093\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0277\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0481\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0160\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0050\n",
      "Epoch [7/10], Step [500/600], Loss: 0.0072\n",
      "Epoch [7/10], Step [600/600], Loss: 0.0015\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0114\n",
      "Epoch [8/10], Step [200/600], Loss: 0.0048\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0161\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0316\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0070\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0099\n",
      "Epoch [9/10], Step [100/600], Loss: 0.0228\n",
      "Epoch [9/10], Step [200/600], Loss: 0.0139\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0199\n",
      "Epoch [9/10], Step [400/600], Loss: 0.0070\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0096\n",
      "Epoch [9/10], Step [600/600], Loss: 0.0153\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0052\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0030\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0041\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0060\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0087\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0121\n"
     ]
    }
   ],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf36b44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.89999999999999 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = len(test_loader.dataset)\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the {n_samples} test images: {100*acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d01ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(imgs):\n",
    "    imgs = imgs / 2 + 0.5   # unnormalize\n",
    "    npimgs = imgs.numpy()\n",
    "    plt.imshow(np.transpose(npimgs, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# one batch of random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "img_grid = torchvision.utils.make_grid(images[0:25], nrow=5)\n",
    "imshow(img_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
