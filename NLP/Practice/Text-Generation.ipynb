{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298556e7",
   "metadata": {},
   "source": [
    "# Using Transformers and a pretrained language model capable of generating text (e.g., GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc98bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450efd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFOpenAIGPTLMHeadModel: ['h.2.attn.bias', 'h.8.attn.bias', 'h.7.attn.bias', 'h.0.attn.bias', 'h.11.attn.bias', 'h.9.attn.bias', 'h.6.attn.bias', 'h.4.attn.bias', 'h.10.attn.bias', 'h.5.attn.bias', 'h.3.attn.bias', 'h.1.attn.bias']\n",
      "- This IS expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFOpenAIGPTLMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFOpenAIGPTLMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5206816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64b8dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3570, 1473], 'attention_mask': [1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer(\"hello everyone\"))\n",
    "\n",
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a30411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   481,  1632,  1807,   498,   618, 22170,   239,\n",
       "          498,   481,  4031,   240,   664,  1872,  1252,   562, 11665,\n",
       "          240,   504,   481, 19995,   498,   734,  4646,   240, 38531,\n",
       "         5466,   240,   488,  6151, 39276,   240,   504,   481, 12191,\n",
       "          498,   734,  4646,   488,  6151],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,   488,   507,   544,   603,   525,   507,   636,\n",
       "          580,  1347, 14306,  2510,   522,   481,    11,  1060,   552,\n",
       "        10707,   240,   912,   481,  3762,   509,  2160,   655,   485,\n",
       "         3013,   481,  6848,   485,    10,   960,   799,   239,   256,\n",
       "        40477,   481,  1122,   618,   816],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   939,   485,   580,  1816,   557,  6826,   535,   637,\n",
       "           11,   944,   562,   481, 13593,  1424, 33358,   498,  6826,\n",
       "          240,   488,   485,   256,   481,  2056, 19461,   498,   618,\n",
       "        36275,   256,   239, 36275,   535,   945,  3891,   481,  8803,\n",
       "         1279,   509,  1359,   481,   618],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239,   507,  2763,   500,  5686, 20919,   239,   507,\n",
       "          544,   531, 10155,   525,   812,   604,   485,   580,  4673,\n",
       "          556,  1533,   488, 26391,   239,   597,   525,   512,   488,\n",
       "          249,   640,   817,   485,  1973,   240,   606,  1259,  3226,\n",
       "          622,  4867,   239,   249,   812],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   984, 11093,   715,   481,  6053, 14047,   500,\n",
       "         1099,  1983,   271,   481,  1454, 16187,   240,   525,   509,\n",
       "         4049,  2034,   507,   240,   488,   481, 26584,   270,   481,\n",
       "         3654, 16187,   240,   525,   509,  4049,   491,   481, 10687,\n",
       "          488,  3360, 11152,   270,   488]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b8aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle, the lost city of king stor. of the ships, no longer used for sailing, on the shores of parri, aeneas, and sherzad, on the seas of parri and sher\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. and it is said that it would be called dragonwood or the idealethorn, because the bridge was placed there to protect the passage to daligo.'\n",
      " the old king looked\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle came to be known as england's soiree for the upcoming great danes of england, and to'the green realms of king canute '. canute's son richard the lionheart was also the king\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. it lies in grave peril. it is an offense that will have to be treated with care and deference. now that you and i are able to meet, we must begin our journey. i will\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, which ruled over the fourth continent in every form : the white isle, that was named upon it, and the trident ; the east isle, that was named at the sixth and final wars ; and\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
