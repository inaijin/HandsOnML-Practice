{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861d5834",
   "metadata": {},
   "source": [
    "# Using a Double Dueling DQN to train an agent that can achieve a superhuman level at the Atari Breakout game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28430788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b19265e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "env = gym.make(\"ALE/Breakout-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8653b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvName = \"ALE/Breakout-v5\"\n",
    "Transition = namedtuple(\"Transition\", (\"State\", \"Action\", \"Reward\", \"NextState\", \"Done\"))\n",
    "\n",
    "StackSize = 4                   # number of frames per state\n",
    "InputHeight = 84                # height after crop+resize\n",
    "InputWidth = 84                 # width after crop+resize\n",
    "ReplayBufferSize = 100000\n",
    "Gamma = 0.99\n",
    "LearningRate = 2.5e-4\n",
    "MaxStepsPerEpisode = 10000\n",
    "ClipRewards = True              # clip rewards to {-1, 0, 1}\n",
    "\n",
    "MaxEpisodes = 200              # hard limit\n",
    "MinReplaySize = 1000           # start training sooner\n",
    "TrainEveryNSteps = 2           # train more often\n",
    "BatchSize = 64                 # larger batches\n",
    "TargetUpdateEvery = 500        # update target net more frequently\n",
    "\n",
    "EpsilonStart = 1.0\n",
    "EpsilonEnd = 0.05\n",
    "EpsilonDecayFrames = MaxEpisodes * MaxStepsPerEpisode // 1.5  # reach min by ~episode 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5528fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessings\n",
    "\n",
    "def PreprocessFrame(frame):\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # TODO: (If needed) Adjust crop indices if visually needed. Here: rows 34:194 is common (160 high).\n",
    "    # crop â€” for Breakout typical useful crop: remove top black border and bottom score area.\n",
    "    cropped = gray[34:194, :]  # H:160 x W:160 for standard 210x160 input; adapt if env different\n",
    "\n",
    "    resized = cv2.resize(cropped, (InputWidth, InputHeight), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized\n",
    "\n",
    "def StackFrames(frameDeque, frame, isNewEpisode):\n",
    "    processed = PreprocessFrame(frame)\n",
    "    if isNewEpisode:\n",
    "        # at new episode replicate same frame StackSize times\n",
    "        frameDeque.clear()\n",
    "        for _ in range(StackSize):\n",
    "            frameDeque.append(processed)\n",
    "    else:\n",
    "        frameDeque.append(processed)\n",
    "\n",
    "    stacked = np.stack(frameDeque, axis=-1)  # shape H x W x StackSize, dtype uint8\n",
    "    return np.array(stacked, dtype=np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb7bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Classes\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def Push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "    def Sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = np.array([t.State for t in batch], dtype=np.float32)\n",
    "        actions = np.array([t.Action for t in batch], dtype=np.int32)\n",
    "        rewards = np.array([t.Reward for t in batch], dtype=np.float32)\n",
    "        next_states = np.array([t.NextState for t in batch], dtype=np.float32)\n",
    "        dones = np.array([t.Done for t in batch], dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "        self.frameCount = 0\n",
    "\n",
    "    def GetEpsilon(self):\n",
    "        fraction = min(float(self.frameCount) / float(EpsilonDecayFrames), 1.0)\n",
    "        eps = EpsilonStart + fraction * (EpsilonEnd - EpsilonStart)  # from 1.0 to 0.05\n",
    "        return eps\n",
    "\n",
    "    def SelectAction(self, model: tf.keras.Model, state: np.ndarray):\n",
    "        self.frameCount += 1\n",
    "        if random.random() < self.GetEpsilon():\n",
    "            return random.randrange(self.n_actions)\n",
    "        stateInput = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "        qvals = model.predict(stateInput, verbose=0)[0]\n",
    "        return int(np.argmax(qvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c21390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "\n",
    "def BuildDuelingDqn(input_shape, n_actions):\n",
    "    inputs = layers.Input(shape=input_shape, name=\"InputFrames\")\n",
    "\n",
    "    # Convolutional feature extractor (Nature DQN)\n",
    "    x = layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\")(inputs)\n",
    "    x = layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides=1, activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "\n",
    "    # Dueling streams\n",
    "    value = layers.Dense(1, name=\"StateValue\")(x)                 # V(s)\n",
    "    advantage = layers.Dense(n_actions, name=\"RawAdvantages\")(x)  # A(s,a)\n",
    "\n",
    "    # mean(A) across actions as a Keras op\n",
    "    advantageMean = layers.Lambda(\n",
    "        lambda a: tf.reduce_mean(a, axis=1, keepdims=True),\n",
    "        output_shape=(1,)\n",
    "    )(advantage)\n",
    "\n",
    "    advCentered = layers.Subtract()([advantage, advantageMean])\n",
    "    qValues = layers.Add(name=\"QValues\")([value, advCentered])\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=qValues)\n",
    "\n",
    "def PlayOneStep(env, agent, frameDeque, state):\n",
    "    action = agent.policy.SelectAction(agent.onlineModel, state)\n",
    "    nextFrame, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if ClipRewards:\n",
    "        clippedReward = np.sign(reward)\n",
    "    else:\n",
    "        clippedReward = reward\n",
    "\n",
    "    nextState = StackFrames(frameDeque, nextFrame, False)\n",
    "\n",
    "    done = bool(terminated or truncated)\n",
    "    return nextState, clippedReward, done, info, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0005dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, env: gym.Env):\n",
    "        self.env = env\n",
    "        self.nActions = env.action_space.n\n",
    "        self.inputShape = (InputHeight, InputWidth, StackSize)\n",
    "        self.onlineModel = BuildDuelingDqn(self.inputShape, self.nActions)\n",
    "        self.targetModel = BuildDuelingDqn(self.inputShape, self.nActions)\n",
    "        self.targetModel.set_weights(self.onlineModel.get_weights())\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LearningRate)\n",
    "        self.lossFn = tf.keras.losses.Huber()   # stable than MSE\n",
    "        self.replayBuffer = ReplayBuffer(ReplayBufferSize)\n",
    "        self.policy = EpsilonGreedyPolicy(self.nActions)\n",
    "        self.trainStepCounter = 0\n",
    "\n",
    "    def UpdateTargetNetwork(self):\n",
    "        self.targetModel.set_weights(self.onlineModel.get_weights())\n",
    "\n",
    "    def TrainStep(self):\n",
    "        if len(self.replayBuffer) < BatchSize:\n",
    "            return 0.0\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replayBuffer.Sample(BatchSize)\n",
    "\n",
    "        # Convert to tensors\n",
    "        statesTensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        nextStatesTensor = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actionsTensor = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewardsTensor = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        donesTensor = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        # Double DQN target computation:\n",
    "        # a_max = argmax_a Q_online(next_state, a)\n",
    "        # target_q = r + gamma * Q_target(next_state, a_max) * (1 - done)\n",
    "        # Use tape to compute online prediction gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            # predicted Q for chosen actions from online network\n",
    "            qValues = self.onlineModel(statesTensor, training=True)\n",
    "            batchIndices = tf.range(tf.shape(qValues)[0])\n",
    "            # q for actions taken\n",
    "            qTaken = tf.gather_nd(qValues, tf.stack([batchIndices, actionsTensor], axis=1))\n",
    "\n",
    "            # compute next actions from online network\n",
    "            qNextOnline = self.onlineModel(nextStatesTensor, training=False)\n",
    "            nextActions = tf.argmax(qNextOnline, axis=1, output_type=tf.int32)\n",
    "\n",
    "            # compute Q-values of next states from target network and pick according to nextActions\n",
    "            qNextTarget = self.targetModel(nextStatesTensor, training=False)\n",
    "            qNextTargetChosen = tf.gather_nd(qNextTarget, tf.stack([batchIndices, nextActions], axis=1))\n",
    "\n",
    "            # backup targets\n",
    "            targets = rewardsTensor + (1.0 - donesTensor) * Gamma * qNextTargetChosen\n",
    "\n",
    "            # loss\n",
    "            loss = self.lossFn(targets, qTaken)\n",
    "\n",
    "        grads = tape.gradient(loss, self.onlineModel.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.onlineModel.trainable_variables))\n",
    "\n",
    "        self.trainStepCounter += 1\n",
    "        return float(loss)\n",
    "\n",
    "    def SaveModel(self, path: str):\n",
    "        self.onlineModel.save_weights(path)\n",
    "\n",
    "    def LoadModel(self, path: str):\n",
    "        self.onlineModel.load_weights(path)\n",
    "        self.targetModel.set_weights(self.onlineModel.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264e2de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1 | Steps  136 | EpisodeReward    0.0 | Avg100   0.00 | ReplaySize    136 | Eps 1.000\n",
      "Episode    2 | Steps  137 | EpisodeReward    0.0 | Avg100   0.00 | ReplaySize    273 | Eps 1.000\n",
      "Episode    3 | Steps  183 | EpisodeReward    1.0 | Avg100   0.33 | ReplaySize    456 | Eps 1.000\n",
      "Episode    4 | Steps  228 | EpisodeReward    3.0 | Avg100   1.00 | ReplaySize    684 | Eps 1.000\n",
      "Episode    5 | Steps  134 | EpisodeReward    0.0 | Avg100   0.80 | ReplaySize    818 | Eps 0.999\n",
      "Episode    6 | Steps  188 | EpisodeReward    1.0 | Avg100   0.83 | ReplaySize   1006 | Eps 0.999\n",
      "Episode    7 | Steps  276 | EpisodeReward    3.0 | Avg100   1.14 | ReplaySize   1282 | Eps 0.999\n",
      "Episode    8 | Steps  223 | EpisodeReward    3.0 | Avg100   1.38 | ReplaySize   1505 | Eps 0.999\n",
      "Episode    9 | Steps  283 | EpisodeReward    3.0 | Avg100   1.56 | ReplaySize   1788 | Eps 0.999\n",
      "Episode   10 | Steps  225 | EpisodeReward    2.0 | Avg100   1.60 | ReplaySize   2013 | Eps 0.999\n",
      "Episode   11 | Steps  140 | EpisodeReward    0.0 | Avg100   1.45 | ReplaySize   2153 | Eps 0.998\n",
      "Episode   12 | Steps  258 | EpisodeReward    3.0 | Avg100   1.58 | ReplaySize   2411 | Eps 0.998\n",
      "Episode   13 | Steps  187 | EpisodeReward    1.0 | Avg100   1.54 | ReplaySize   2598 | Eps 0.998\n",
      "Episode   14 | Steps  140 | EpisodeReward    0.0 | Avg100   1.43 | ReplaySize   2738 | Eps 0.998\n",
      "Episode   15 | Steps  289 | EpisodeReward    4.0 | Avg100   1.60 | ReplaySize   3027 | Eps 0.998\n",
      "Episode   16 | Steps  140 | EpisodeReward    0.0 | Avg100   1.50 | ReplaySize   3167 | Eps 0.998\n",
      "Episode   17 | Steps  175 | EpisodeReward    1.0 | Avg100   1.47 | ReplaySize   3342 | Eps 0.998\n",
      "Episode   18 | Steps  174 | EpisodeReward    1.0 | Avg100   1.44 | ReplaySize   3516 | Eps 0.997\n",
      "Episode   19 | Steps  227 | EpisodeReward    2.0 | Avg100   1.47 | ReplaySize   3743 | Eps 0.997\n",
      "Episode   20 | Steps  274 | EpisodeReward    3.0 | Avg100   1.55 | ReplaySize   4017 | Eps 0.997\n",
      "Episode   21 | Steps  155 | EpisodeReward    1.0 | Avg100   1.52 | ReplaySize   4172 | Eps 0.997\n",
      "Episode   22 | Steps  256 | EpisodeReward    3.0 | Avg100   1.59 | ReplaySize   4428 | Eps 0.997\n",
      "Episode   23 | Steps  224 | EpisodeReward    2.0 | Avg100   1.61 | ReplaySize   4652 | Eps 0.997\n",
      "Episode   24 | Steps  130 | EpisodeReward    0.0 | Avg100   1.54 | ReplaySize   4782 | Eps 0.997\n",
      "Episode   25 | Steps  184 | EpisodeReward    1.0 | Avg100   1.52 | ReplaySize   4966 | Eps 0.996\n",
      "Episode   26 | Steps  139 | EpisodeReward    0.0 | Avg100   1.46 | ReplaySize   5105 | Eps 0.996\n",
      "Episode   27 | Steps  132 | EpisodeReward    0.0 | Avg100   1.41 | ReplaySize   5237 | Eps 0.996\n",
      "Episode   28 | Steps  178 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize   5415 | Eps 0.996\n",
      "Episode   29 | Steps  197 | EpisodeReward    1.0 | Avg100   1.38 | ReplaySize   5612 | Eps 0.996\n",
      "Episode   30 | Steps  178 | EpisodeReward    1.0 | Avg100   1.37 | ReplaySize   5790 | Eps 0.996\n",
      "Episode   31 | Steps  183 | EpisodeReward    1.0 | Avg100   1.35 | ReplaySize   5973 | Eps 0.996\n",
      "Episode   32 | Steps  133 | EpisodeReward    0.0 | Avg100   1.31 | ReplaySize   6106 | Eps 0.996\n",
      "Episode   33 | Steps  202 | EpisodeReward    2.0 | Avg100   1.33 | ReplaySize   6308 | Eps 0.996\n",
      "Episode   34 | Steps  296 | EpisodeReward    4.0 | Avg100   1.41 | ReplaySize   6604 | Eps 0.995\n",
      "Episode   35 | Steps  202 | EpisodeReward    2.0 | Avg100   1.43 | ReplaySize   6806 | Eps 0.995\n",
      "Episode   36 | Steps  222 | EpisodeReward    2.0 | Avg100   1.44 | ReplaySize   7028 | Eps 0.995\n",
      "Episode   37 | Steps  194 | EpisodeReward    2.0 | Avg100   1.46 | ReplaySize   7222 | Eps 0.995\n",
      "Episode   38 | Steps  180 | EpisodeReward    1.0 | Avg100   1.45 | ReplaySize   7402 | Eps 0.995\n",
      "Episode   39 | Steps  235 | EpisodeReward    2.0 | Avg100   1.46 | ReplaySize   7637 | Eps 0.995\n",
      "Episode   40 | Steps  194 | EpisodeReward    1.0 | Avg100   1.45 | ReplaySize   7831 | Eps 0.994\n",
      "Episode   41 | Steps  237 | EpisodeReward    3.0 | Avg100   1.49 | ReplaySize   8068 | Eps 0.994\n",
      "Episode   42 | Steps  158 | EpisodeReward    1.0 | Avg100   1.48 | ReplaySize   8226 | Eps 0.994\n",
      "Episode   43 | Steps  273 | EpisodeReward    3.0 | Avg100   1.51 | ReplaySize   8499 | Eps 0.994\n",
      "Episode   44 | Steps  130 | EpisodeReward    0.0 | Avg100   1.48 | ReplaySize   8629 | Eps 0.994\n",
      "Episode   45 | Steps  173 | EpisodeReward    1.0 | Avg100   1.47 | ReplaySize   8802 | Eps 0.994\n",
      "Episode   46 | Steps  133 | EpisodeReward    0.0 | Avg100   1.43 | ReplaySize   8935 | Eps 0.994\n",
      "Episode   47 | Steps  227 | EpisodeReward    2.0 | Avg100   1.45 | ReplaySize   9162 | Eps 0.993\n",
      "Episode   48 | Steps  228 | EpisodeReward    2.0 | Avg100   1.46 | ReplaySize   9390 | Eps 0.993\n",
      "Episode   49 | Steps  156 | EpisodeReward    0.0 | Avg100   1.43 | ReplaySize   9546 | Eps 0.993\n",
      "Episode   50 | Steps  130 | EpisodeReward    0.0 | Avg100   1.40 | ReplaySize   9676 | Eps 0.993\n",
      "Episode   51 | Steps  179 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize   9855 | Eps 0.993\n",
      "Episode   52 | Steps  306 | EpisodeReward    4.0 | Avg100   1.44 | ReplaySize  10161 | Eps 0.993\n",
      "Episode   53 | Steps  279 | EpisodeReward    3.0 | Avg100   1.47 | ReplaySize  10440 | Eps 0.993\n",
      "Episode   54 | Steps  229 | EpisodeReward    2.0 | Avg100   1.48 | ReplaySize  10669 | Eps 0.992\n",
      "Episode   55 | Steps  317 | EpisodeReward    5.0 | Avg100   1.55 | ReplaySize  10986 | Eps 0.992\n",
      "Episode   56 | Steps  186 | EpisodeReward    1.0 | Avg100   1.54 | ReplaySize  11172 | Eps 0.992\n",
      "Episode   57 | Steps  139 | EpisodeReward    0.0 | Avg100   1.51 | ReplaySize  11311 | Eps 0.992\n",
      "Episode   58 | Steps  280 | EpisodeReward    3.0 | Avg100   1.53 | ReplaySize  11591 | Eps 0.992\n",
      "Episode   59 | Steps  271 | EpisodeReward    3.0 | Avg100   1.56 | ReplaySize  11862 | Eps 0.992\n",
      "Episode   60 | Steps  275 | EpisodeReward    4.0 | Avg100   1.60 | ReplaySize  12137 | Eps 0.991\n",
      "Episode   61 | Steps  232 | EpisodeReward    2.0 | Avg100   1.61 | ReplaySize  12369 | Eps 0.991\n",
      "Episode   62 | Steps  216 | EpisodeReward    2.0 | Avg100   1.61 | ReplaySize  12585 | Eps 0.991\n",
      "Episode   63 | Steps  128 | EpisodeReward    0.0 | Avg100   1.59 | ReplaySize  12713 | Eps 0.991\n",
      "Episode   64 | Steps  231 | EpisodeReward    2.0 | Avg100   1.59 | ReplaySize  12944 | Eps 0.991\n",
      "Episode   65 | Steps  187 | EpisodeReward    2.0 | Avg100   1.60 | ReplaySize  13131 | Eps 0.991\n",
      "Episode   66 | Steps  155 | EpisodeReward    1.0 | Avg100   1.59 | ReplaySize  13286 | Eps 0.991\n",
      "Episode   67 | Steps  157 | EpisodeReward    1.0 | Avg100   1.58 | ReplaySize  13443 | Eps 0.990\n",
      "Episode   68 | Steps  150 | EpisodeReward    0.0 | Avg100   1.56 | ReplaySize  13593 | Eps 0.990\n",
      "Episode   69 | Steps  198 | EpisodeReward    1.0 | Avg100   1.55 | ReplaySize  13791 | Eps 0.990\n",
      "Episode   70 | Steps  136 | EpisodeReward    0.0 | Avg100   1.53 | ReplaySize  13927 | Eps 0.990\n",
      "Episode   71 | Steps  179 | EpisodeReward    1.0 | Avg100   1.52 | ReplaySize  14106 | Eps 0.990\n",
      "Episode   72 | Steps  244 | EpisodeReward    2.0 | Avg100   1.53 | ReplaySize  14350 | Eps 0.990\n",
      "Episode   73 | Steps  283 | EpisodeReward    3.0 | Avg100   1.55 | ReplaySize  14633 | Eps 0.990\n",
      "Episode   74 | Steps  205 | EpisodeReward    2.0 | Avg100   1.55 | ReplaySize  14838 | Eps 0.989\n",
      "Episode   75 | Steps  174 | EpisodeReward    1.0 | Avg100   1.55 | ReplaySize  15012 | Eps 0.989\n",
      "Episode   76 | Steps  145 | EpisodeReward    0.0 | Avg100   1.53 | ReplaySize  15157 | Eps 0.989\n",
      "Episode   77 | Steps  195 | EpisodeReward    2.0 | Avg100   1.53 | ReplaySize  15352 | Eps 0.989\n",
      "Episode   78 | Steps  134 | EpisodeReward    0.0 | Avg100   1.51 | ReplaySize  15486 | Eps 0.989\n",
      "Episode   79 | Steps  334 | EpisodeReward    5.0 | Avg100   1.56 | ReplaySize  15820 | Eps 0.989\n",
      "Episode   80 | Steps  137 | EpisodeReward    0.0 | Avg100   1.54 | ReplaySize  15957 | Eps 0.989\n",
      "Episode   81 | Steps  167 | EpisodeReward    1.0 | Avg100   1.53 | ReplaySize  16124 | Eps 0.989\n",
      "Episode   82 | Steps  225 | EpisodeReward    2.0 | Avg100   1.54 | ReplaySize  16349 | Eps 0.988\n",
      "Episode   83 | Steps  207 | EpisodeReward    2.0 | Avg100   1.54 | ReplaySize  16556 | Eps 0.988\n",
      "Episode   84 | Steps  175 | EpisodeReward    1.0 | Avg100   1.54 | ReplaySize  16731 | Eps 0.988\n",
      "Episode   85 | Steps  150 | EpisodeReward    0.0 | Avg100   1.52 | ReplaySize  16881 | Eps 0.988\n",
      "Episode   86 | Steps  132 | EpisodeReward    0.0 | Avg100   1.50 | ReplaySize  17013 | Eps 0.988\n",
      "Episode   87 | Steps  227 | EpisodeReward    2.0 | Avg100   1.51 | ReplaySize  17240 | Eps 0.988\n",
      "Episode   88 | Steps  133 | EpisodeReward    0.0 | Avg100   1.49 | ReplaySize  17373 | Eps 0.988\n",
      "Episode   89 | Steps  169 | EpisodeReward    1.0 | Avg100   1.48 | ReplaySize  17542 | Eps 0.988\n",
      "Episode   90 | Steps  126 | EpisodeReward    0.0 | Avg100   1.47 | ReplaySize  17668 | Eps 0.987\n",
      "Episode   91 | Steps  156 | EpisodeReward    1.0 | Avg100   1.46 | ReplaySize  17824 | Eps 0.987\n",
      "Episode   92 | Steps  210 | EpisodeReward    2.0 | Avg100   1.47 | ReplaySize  18034 | Eps 0.987\n",
      "Episode   93 | Steps  170 | EpisodeReward    1.0 | Avg100   1.46 | ReplaySize  18204 | Eps 0.987\n",
      "Episode   94 | Steps  210 | EpisodeReward    2.0 | Avg100   1.47 | ReplaySize  18414 | Eps 0.987\n",
      "Episode   95 | Steps  147 | EpisodeReward    0.0 | Avg100   1.45 | ReplaySize  18561 | Eps 0.987\n",
      "Episode   96 | Steps  296 | EpisodeReward    4.0 | Avg100   1.48 | ReplaySize  18857 | Eps 0.987\n",
      "Episode   97 | Steps  139 | EpisodeReward    0.0 | Avg100   1.46 | ReplaySize  18996 | Eps 0.986\n",
      "Episode   98 | Steps  135 | EpisodeReward    0.0 | Avg100   1.45 | ReplaySize  19131 | Eps 0.986\n",
      "Episode   99 | Steps  156 | EpisodeReward    1.0 | Avg100   1.44 | ReplaySize  19287 | Eps 0.986\n",
      "Episode  100 | Steps  125 | EpisodeReward    0.0 | Avg100   1.43 | ReplaySize  19412 | Eps 0.986\n",
      "Episode  101 | Steps  187 | EpisodeReward    2.0 | Avg100   1.45 | ReplaySize  19599 | Eps 0.986\n",
      "Episode  102 | Steps  226 | EpisodeReward    2.0 | Avg100   1.47 | ReplaySize  19825 | Eps 0.986\n",
      "Episode  103 | Steps  203 | EpisodeReward    2.0 | Avg100   1.48 | ReplaySize  20028 | Eps 0.986\n",
      "Episode  104 | Steps  139 | EpisodeReward    0.0 | Avg100   1.45 | ReplaySize  20167 | Eps 0.986\n",
      "Episode  105 | Steps  126 | EpisodeReward    0.0 | Avg100   1.45 | ReplaySize  20293 | Eps 0.986\n",
      "Episode  106 | Steps  271 | EpisodeReward    3.0 | Avg100   1.47 | ReplaySize  20564 | Eps 0.985\n",
      "Episode  107 | Steps  153 | EpisodeReward    0.0 | Avg100   1.44 | ReplaySize  20717 | Eps 0.985\n",
      "Episode  108 | Steps  232 | EpisodeReward    2.0 | Avg100   1.43 | ReplaySize  20949 | Eps 0.985\n",
      "Episode  109 | Steps  170 | EpisodeReward    1.0 | Avg100   1.41 | ReplaySize  21119 | Eps 0.985\n",
      "Episode  110 | Steps  281 | EpisodeReward    3.0 | Avg100   1.42 | ReplaySize  21400 | Eps 0.985\n",
      "Episode  111 | Steps  126 | EpisodeReward    0.0 | Avg100   1.42 | ReplaySize  21526 | Eps 0.985\n",
      "Episode  112 | Steps  157 | EpisodeReward    1.0 | Avg100   1.40 | ReplaySize  21683 | Eps 0.985\n",
      "Episode  113 | Steps  193 | EpisodeReward    1.0 | Avg100   1.40 | ReplaySize  21876 | Eps 0.984\n",
      "Episode  114 | Steps  165 | EpisodeReward    1.0 | Avg100   1.41 | ReplaySize  22041 | Eps 0.984\n",
      "Episode  115 | Steps  173 | EpisodeReward    1.0 | Avg100   1.38 | ReplaySize  22214 | Eps 0.984\n",
      "Episode  116 | Steps  176 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize  22390 | Eps 0.984\n",
      "Episode  117 | Steps  171 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize  22561 | Eps 0.984\n",
      "Episode  118 | Steps  133 | EpisodeReward    0.0 | Avg100   1.38 | ReplaySize  22694 | Eps 0.984\n",
      "Episode  119 | Steps  136 | EpisodeReward    0.0 | Avg100   1.36 | ReplaySize  22830 | Eps 0.984\n",
      "Episode  120 | Steps  185 | EpisodeReward    1.0 | Avg100   1.34 | ReplaySize  23015 | Eps 0.984\n",
      "Episode  121 | Steps  175 | EpisodeReward    1.0 | Avg100   1.34 | ReplaySize  23190 | Eps 0.983\n",
      "Episode  122 | Steps  130 | EpisodeReward    0.0 | Avg100   1.31 | ReplaySize  23320 | Eps 0.983\n",
      "Episode  123 | Steps  225 | EpisodeReward    2.0 | Avg100   1.31 | ReplaySize  23545 | Eps 0.983\n",
      "Episode  124 | Steps  188 | EpisodeReward    2.0 | Avg100   1.33 | ReplaySize  23733 | Eps 0.983\n",
      "Episode  125 | Steps  135 | EpisodeReward    0.0 | Avg100   1.32 | ReplaySize  23868 | Eps 0.983\n",
      "Episode  126 | Steps  199 | EpisodeReward    2.0 | Avg100   1.34 | ReplaySize  24067 | Eps 0.983\n",
      "Episode  127 | Steps  159 | EpisodeReward    1.0 | Avg100   1.35 | ReplaySize  24226 | Eps 0.983\n",
      "Episode  128 | Steps  228 | EpisodeReward    2.0 | Avg100   1.36 | ReplaySize  24454 | Eps 0.983\n",
      "Episode  129 | Steps  136 | EpisodeReward    0.0 | Avg100   1.35 | ReplaySize  24590 | Eps 0.982\n",
      "Episode  130 | Steps  216 | EpisodeReward    2.0 | Avg100   1.36 | ReplaySize  24806 | Eps 0.982\n",
      "Episode  131 | Steps  243 | EpisodeReward    2.0 | Avg100   1.37 | ReplaySize  25049 | Eps 0.982\n",
      "Episode  132 | Steps  254 | EpisodeReward    3.0 | Avg100   1.40 | ReplaySize  25303 | Eps 0.982\n",
      "Episode  133 | Steps  192 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize  25495 | Eps 0.982\n",
      "Episode  134 | Steps  341 | EpisodeReward    5.0 | Avg100   1.40 | ReplaySize  25836 | Eps 0.982\n",
      "Episode  135 | Steps  271 | EpisodeReward    3.0 | Avg100   1.41 | ReplaySize  26107 | Eps 0.981\n",
      "Episode  136 | Steps  161 | EpisodeReward    1.0 | Avg100   1.40 | ReplaySize  26268 | Eps 0.981\n",
      "Episode  137 | Steps  307 | EpisodeReward    4.0 | Avg100   1.42 | ReplaySize  26575 | Eps 0.981\n",
      "Episode  138 | Steps  138 | EpisodeReward    0.0 | Avg100   1.41 | ReplaySize  26713 | Eps 0.981\n",
      "Episode  139 | Steps  143 | EpisodeReward    0.0 | Avg100   1.39 | ReplaySize  26856 | Eps 0.981\n",
      "Episode  140 | Steps  154 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize  27010 | Eps 0.981\n",
      "Episode  141 | Steps  272 | EpisodeReward    3.0 | Avg100   1.39 | ReplaySize  27282 | Eps 0.981\n",
      "Episode  142 | Steps  211 | EpisodeReward    2.0 | Avg100   1.40 | ReplaySize  27493 | Eps 0.980\n",
      "Episode  143 | Steps  127 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  27620 | Eps 0.980\n",
      "Episode  144 | Steps  128 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  27748 | Eps 0.980\n",
      "Episode  145 | Steps  210 | EpisodeReward    2.0 | Avg100   1.38 | ReplaySize  27958 | Eps 0.980\n",
      "Episode  146 | Steps  211 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize  28169 | Eps 0.980\n",
      "Episode  147 | Steps  194 | EpisodeReward    1.0 | Avg100   1.38 | ReplaySize  28363 | Eps 0.980\n",
      "Episode  148 | Steps  270 | EpisodeReward    3.0 | Avg100   1.39 | ReplaySize  28633 | Eps 0.980\n",
      "Episode  149 | Steps  283 | EpisodeReward    3.0 | Avg100   1.42 | ReplaySize  28916 | Eps 0.979\n",
      "Episode  150 | Steps  210 | EpisodeReward    2.0 | Avg100   1.44 | ReplaySize  29126 | Eps 0.979\n",
      "Episode  151 | Steps  126 | EpisodeReward    0.0 | Avg100   1.43 | ReplaySize  29252 | Eps 0.979\n",
      "Episode  152 | Steps  170 | EpisodeReward    1.0 | Avg100   1.40 | ReplaySize  29422 | Eps 0.979\n",
      "Episode  153 | Steps  129 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  29551 | Eps 0.979\n",
      "Episode  154 | Steps  182 | EpisodeReward    1.0 | Avg100   1.36 | ReplaySize  29733 | Eps 0.979\n",
      "Episode  155 | Steps  155 | EpisodeReward    0.0 | Avg100   1.31 | ReplaySize  29888 | Eps 0.979\n",
      "Episode  156 | Steps  204 | EpisodeReward    2.0 | Avg100   1.32 | ReplaySize  30092 | Eps 0.979\n",
      "Episode  157 | Steps  179 | EpisodeReward    1.0 | Avg100   1.33 | ReplaySize  30271 | Eps 0.978\n",
      "Episode  158 | Steps  195 | EpisodeReward    2.0 | Avg100   1.32 | ReplaySize  30466 | Eps 0.978\n",
      "Episode  159 | Steps  300 | EpisodeReward    3.0 | Avg100   1.32 | ReplaySize  30766 | Eps 0.978\n",
      "Episode  160 | Steps  135 | EpisodeReward    0.0 | Avg100   1.28 | ReplaySize  30901 | Eps 0.978\n",
      "Episode  161 | Steps  262 | EpisodeReward    3.0 | Avg100   1.29 | ReplaySize  31163 | Eps 0.978\n",
      "Episode  162 | Steps  145 | EpisodeReward    0.0 | Avg100   1.27 | ReplaySize  31308 | Eps 0.978\n",
      "Episode  163 | Steps  207 | EpisodeReward    2.0 | Avg100   1.29 | ReplaySize  31515 | Eps 0.978\n",
      "Episode  164 | Steps  231 | EpisodeReward    3.0 | Avg100   1.30 | ReplaySize  31746 | Eps 0.977\n",
      "Episode  165 | Steps  277 | EpisodeReward    3.0 | Avg100   1.31 | ReplaySize  32023 | Eps 0.977\n",
      "Episode  166 | Steps  244 | EpisodeReward    3.0 | Avg100   1.33 | ReplaySize  32267 | Eps 0.977\n",
      "Episode  167 | Steps  191 | EpisodeReward    1.0 | Avg100   1.33 | ReplaySize  32458 | Eps 0.977\n",
      "Episode  168 | Steps  173 | EpisodeReward    1.0 | Avg100   1.34 | ReplaySize  32631 | Eps 0.977\n",
      "Episode  169 | Steps  199 | EpisodeReward    2.0 | Avg100   1.35 | ReplaySize  32830 | Eps 0.977\n",
      "Episode  170 | Steps  215 | EpisodeReward    2.0 | Avg100   1.37 | ReplaySize  33045 | Eps 0.976\n",
      "Episode  171 | Steps  181 | EpisodeReward    1.0 | Avg100   1.37 | ReplaySize  33226 | Eps 0.976\n",
      "Episode  172 | Steps  254 | EpisodeReward    3.0 | Avg100   1.38 | ReplaySize  33480 | Eps 0.976\n",
      "Episode  173 | Steps  230 | EpisodeReward    2.0 | Avg100   1.37 | ReplaySize  33710 | Eps 0.976\n",
      "Episode  174 | Steps  271 | EpisodeReward    3.0 | Avg100   1.38 | ReplaySize  33981 | Eps 0.976\n",
      "Episode  175 | Steps  133 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  34114 | Eps 0.976\n",
      "Episode  176 | Steps  170 | EpisodeReward    1.0 | Avg100   1.38 | ReplaySize  34284 | Eps 0.976\n",
      "Episode  177 | Steps  261 | EpisodeReward    3.0 | Avg100   1.39 | ReplaySize  34545 | Eps 0.975\n",
      "Episode  178 | Steps  205 | EpisodeReward    2.0 | Avg100   1.41 | ReplaySize  34750 | Eps 0.975\n",
      "Episode  179 | Steps  130 | EpisodeReward    0.0 | Avg100   1.36 | ReplaySize  34880 | Eps 0.975\n",
      "Episode  180 | Steps  217 | EpisodeReward    2.0 | Avg100   1.38 | ReplaySize  35097 | Eps 0.975\n",
      "Episode  181 | Steps  227 | EpisodeReward    2.0 | Avg100   1.39 | ReplaySize  35324 | Eps 0.975\n",
      "Episode  182 | Steps  197 | EpisodeReward    2.0 | Avg100   1.39 | ReplaySize  35521 | Eps 0.975\n",
      "Episode  183 | Steps  126 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  35647 | Eps 0.975\n",
      "Episode  184 | Steps  184 | EpisodeReward    2.0 | Avg100   1.38 | ReplaySize  35831 | Eps 0.974\n",
      "Episode  185 | Steps  128 | EpisodeReward    0.0 | Avg100   1.38 | ReplaySize  35959 | Eps 0.974\n",
      "Episode  186 | Steps  156 | EpisodeReward    1.0 | Avg100   1.39 | ReplaySize  36115 | Eps 0.974\n",
      "Episode  187 | Steps  130 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  36245 | Eps 0.974\n",
      "Episode  188 | Steps  142 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  36387 | Eps 0.974\n",
      "Episode  189 | Steps  256 | EpisodeReward    4.0 | Avg100   1.40 | ReplaySize  36643 | Eps 0.974\n",
      "Episode  190 | Steps  141 | EpisodeReward    0.0 | Avg100   1.40 | ReplaySize  36784 | Eps 0.974\n",
      "Episode  191 | Steps  182 | EpisodeReward    1.0 | Avg100   1.40 | ReplaySize  36966 | Eps 0.974\n",
      "Episode  192 | Steps  139 | EpisodeReward    0.0 | Avg100   1.38 | ReplaySize  37105 | Eps 0.974\n",
      "Episode  193 | Steps  125 | EpisodeReward    0.0 | Avg100   1.37 | ReplaySize  37230 | Eps 0.973\n",
      "Episode  194 | Steps  314 | EpisodeReward    4.0 | Avg100   1.39 | ReplaySize  37544 | Eps 0.973\n",
      "Episode  195 | Steps  139 | EpisodeReward    0.0 | Avg100   1.39 | ReplaySize  37683 | Eps 0.973\n",
      "Episode  196 | Steps  332 | EpisodeReward    5.0 | Avg100   1.40 | ReplaySize  38015 | Eps 0.973\n",
      "Episode  197 | Steps  133 | EpisodeReward    0.0 | Avg100   1.40 | ReplaySize  38148 | Eps 0.973\n",
      "Episode  198 | Steps  128 | EpisodeReward    0.0 | Avg100   1.40 | ReplaySize  38276 | Eps 0.973\n",
      "Episode  199 | Steps  133 | EpisodeReward    0.0 | Avg100   1.39 | ReplaySize  38409 | Eps 0.973\n",
      "Episode  200 | Steps  271 | EpisodeReward    3.0 | Avg100   1.42 | ReplaySize  38680 | Eps 0.972\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(EnvName, render_mode=None)\n",
    "agent = DqnAgent(env)\n",
    "frameDeque = deque(maxlen=StackSize)\n",
    "\n",
    "totalEnvSteps = 0\n",
    "losses = []\n",
    "episodeRewards = []\n",
    "\n",
    "for episode in range(1, MaxEpisodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    state = StackFrames(frameDeque, obs, True)\n",
    "    episodeReward = 0.0\n",
    "\n",
    "    for step in range(1, MaxStepsPerEpisode + 1):\n",
    "        # interact\n",
    "        nextState, reward, done, info, action = PlayOneStep(env, agent, frameDeque, state)\n",
    "        agent.replayBuffer.Push(state, action, reward, nextState, float(done))\n",
    "        episodeReward += reward\n",
    "        state = nextState\n",
    "        totalEnvSteps += 1\n",
    "\n",
    "        # After enough transitions, train every TrainEveryNSteps\n",
    "        if totalEnvSteps > MinReplaySize and totalEnvSteps % TrainEveryNSteps == 0:\n",
    "            loss = agent.TrainStep()\n",
    "            losses.append(loss)\n",
    "\n",
    "        # update target network periodically\n",
    "        if totalEnvSteps % TargetUpdateEvery == 0:\n",
    "            agent.UpdateTargetNetwork()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episodeRewards.append(episodeReward)\n",
    "    avgReward = np.mean(episodeRewards[-100:])\n",
    "    print(\n",
    "        f\"Episode {episode:4d} | Steps {step:4d} | EpisodeReward {episodeReward:6.1f} \"\n",
    "        f\"| Avg100 {avgReward:6.2f} | ReplaySize {len(agent.replayBuffer):6d} \"\n",
    "        f\"| Eps {agent.policy.GetEpsilon():.3f}\"\n",
    "    )\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        agent.SaveModel(f\"checkpoints/dueling_dqn_ep{episode}.weights.h5\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47a5ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation ===\n",
      "Eval Episode 1: Reward 5.0\n",
      "Eval Episode 2: Reward 0.0\n",
      "Eval Episode 3: Reward 3.0\n",
      "Eval Episode 4: Reward 0.0\n",
      "Eval Episode 5: Reward 0.0\n",
      "\n",
      "Average Eval Reward over 5 episodes: 1.60\n"
     ]
    }
   ],
   "source": [
    "def EvaluateAgent(env, agent, nEpisodes=5):\n",
    "    totalReward = 0\n",
    "    for ep in range(nEpisodes):\n",
    "        obs, _ = env.reset()\n",
    "        frameDeque = deque(maxlen=StackSize)\n",
    "        state = StackFrames(frameDeque, obs, True)\n",
    "        done = False\n",
    "        episodeReward = 0\n",
    "        while not done:\n",
    "            action = agent.policy.SelectAction(agent.onlineModel, state)\n",
    "            nextObs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = StackFrames(frameDeque, nextObs, False)\n",
    "            episodeReward += reward\n",
    "        totalReward += episodeReward\n",
    "        print(f\"Eval Episode {ep+1}: Reward {episodeReward}\")\n",
    "    print(f\"\\nAverage Eval Reward over {nEpisodes} episodes: {totalReward / nEpisodes:.2f}\")\n",
    "\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "EvaluateAgent(gym.make(EnvName, render_mode=None), agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cd22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation with Visualization ===\n",
      "Eval Episode 1 Step 1: Action 1\n",
      "Eval Episode 1 Step 2: Action 0\n",
      "Eval Episode 1 Step 3: Action 0\n",
      "Eval Episode 1 Step 4: Action 1\n",
      "Eval Episode 1 Step 5: Action 1\n",
      "Eval Episode 1 Step 6: Action 0\n",
      "Eval Episode 1 Step 7: Action 0\n",
      "Eval Episode 1 Step 8: Action 1\n",
      "Eval Episode 1 Step 9: Action 0\n",
      "Eval Episode 1 Step 10: Action 0\n",
      "Eval Episode 1 Step 11: Action 1\n",
      "Eval Episode 1 Step 12: Action 1\n",
      "Eval Episode 1 Step 13: Action 0\n",
      "Eval Episode 1 Step 14: Action 0\n",
      "Eval Episode 1 Step 15: Action 2\n",
      "Eval Episode 1 Step 16: Action 2\n",
      "Eval Episode 1 Step 17: Action 2\n",
      "Eval Episode 1 Step 18: Action 3\n",
      "Eval Episode 1 Step 19: Action 3\n",
      "Eval Episode 1 Step 20: Action 1\n",
      "Eval Episode 1 Step 21: Action 2\n",
      "Eval Episode 1 Step 22: Action 1\n",
      "Eval Episode 1 Step 23: Action 1\n",
      "Eval Episode 1 Step 24: Action 1\n",
      "Eval Episode 1 Step 25: Action 0\n",
      "Eval Episode 1 Step 26: Action 3\n",
      "Eval Episode 1 Step 27: Action 2\n",
      "Eval Episode 1 Step 28: Action 0\n",
      "Eval Episode 1 Step 29: Action 3\n",
      "Eval Episode 1 Step 30: Action 1\n",
      "Eval Episode 1 Step 31: Action 3\n",
      "Eval Episode 1 Step 32: Action 3\n",
      "Eval Episode 1 Step 33: Action 3\n",
      "Eval Episode 1 Step 34: Action 3\n",
      "Eval Episode 1 Step 35: Action 3\n",
      "Eval Episode 1 Step 36: Action 0\n",
      "Eval Episode 1 Step 37: Action 3\n",
      "Eval Episode 1 Step 38: Action 3\n",
      "Eval Episode 1 Step 39: Action 0\n",
      "Eval Episode 1 Step 40: Action 0\n",
      "Eval Episode 1 Step 41: Action 1\n",
      "Eval Episode 1 Step 42: Action 1\n",
      "Eval Episode 1 Step 43: Action 1\n",
      "Eval Episode 1 Step 44: Action 1\n",
      "Eval Episode 1 Step 45: Action 1\n",
      "Eval Episode 1 Step 46: Action 1\n",
      "Eval Episode 1 Step 47: Action 1\n",
      "Eval Episode 1 Step 48: Action 3\n",
      "Eval Episode 1 Step 49: Action 0\n",
      "Eval Episode 1 Step 50: Action 3\n",
      "Eval Episode 1 Step 51: Action 2\n",
      "Eval Episode 1 Step 52: Action 3\n",
      "Eval Episode 1 Step 53: Action 3\n",
      "Eval Episode 1 Step 54: Action 0\n",
      "Eval Episode 1 Step 55: Action 0\n",
      "Eval Episode 1 Step 56: Action 2\n",
      "Eval Episode 1 Step 57: Action 2\n",
      "Eval Episode 1 Step 58: Action 2\n",
      "Eval Episode 1 Step 59: Action 0\n",
      "Eval Episode 1 Step 60: Action 2\n",
      "Eval Episode 1 Step 61: Action 0\n",
      "Eval Episode 1 Step 62: Action 2\n",
      "Eval Episode 1 Step 63: Action 2\n",
      "Eval Episode 1 Step 64: Action 0\n",
      "Eval Episode 1 Step 65: Action 0\n",
      "Eval Episode 1 Step 66: Action 0\n",
      "Eval Episode 1 Step 67: Action 0\n",
      "Eval Episode 1 Step 68: Action 1\n",
      "Eval Episode 1 Step 69: Action 1\n",
      "Eval Episode 1 Step 70: Action 3\n",
      "Eval Episode 1 Step 71: Action 0\n",
      "Eval Episode 1 Step 72: Action 1\n",
      "Eval Episode 1 Step 73: Action 1\n",
      "Eval Episode 1 Step 74: Action 1\n",
      "Eval Episode 1 Step 75: Action 1\n",
      "Eval Episode 1 Step 76: Action 0\n",
      "Eval Episode 1 Step 77: Action 0\n",
      "Eval Episode 1 Step 78: Action 1\n",
      "Eval Episode 1 Step 79: Action 1\n",
      "Eval Episode 1 Step 80: Action 3\n",
      "Eval Episode 1 Step 81: Action 3\n",
      "Eval Episode 1 Step 82: Action 3\n",
      "Eval Episode 1 Step 83: Action 3\n",
      "Eval Episode 1 Step 84: Action 3\n",
      "Eval Episode 1 Step 85: Action 3\n",
      "Eval Episode 1 Step 86: Action 3\n",
      "Eval Episode 1 Step 87: Action 3\n",
      "Eval Episode 1 Step 88: Action 0\n",
      "Eval Episode 1 Step 89: Action 0\n",
      "Eval Episode 1 Step 90: Action 0\n",
      "Eval Episode 1 Step 91: Action 1\n",
      "Eval Episode 1 Step 92: Action 1\n",
      "Eval Episode 1 Step 93: Action 1\n",
      "Eval Episode 1 Step 94: Action 1\n",
      "Eval Episode 1 Step 95: Action 1\n",
      "Eval Episode 1 Step 96: Action 1\n",
      "Eval Episode 1 Step 97: Action 3\n",
      "Eval Episode 1 Step 98: Action 1\n",
      "Eval Episode 1 Step 99: Action 1\n",
      "Eval Episode 1 Step 100: Action 3\n",
      "Eval Episode 1 Step 101: Action 0\n",
      "Eval Episode 1 Step 102: Action 0\n",
      "Eval Episode 1 Step 103: Action 0\n",
      "Eval Episode 1 Step 104: Action 0\n",
      "Eval Episode 1 Step 105: Action 1\n",
      "Eval Episode 1 Step 106: Action 0\n",
      "Eval Episode 1 Step 107: Action 0\n",
      "Eval Episode 1 Step 108: Action 0\n",
      "Eval Episode 1 Step 109: Action 0\n",
      "Eval Episode 1 Step 110: Action 0\n",
      "Eval Episode 1 Step 111: Action 0\n",
      "Eval Episode 1 Step 112: Action 1\n",
      "Eval Episode 1 Step 113: Action 1\n",
      "Eval Episode 1 Step 114: Action 1\n",
      "Eval Episode 1 Step 115: Action 1\n",
      "Eval Episode 1 Step 116: Action 1\n",
      "Eval Episode 1 Step 117: Action 1\n",
      "Eval Episode 1 Step 118: Action 1\n",
      "Eval Episode 1 Step 119: Action 1\n",
      "Eval Episode 1 Step 120: Action 1\n",
      "Eval Episode 1 Step 121: Action 1\n",
      "Eval Episode 1 Step 122: Action 1\n",
      "Eval Episode 1 Step 123: Action 1\n",
      "Eval Episode 1 Step 124: Action 3\n",
      "Eval Episode 1 Step 125: Action 3\n",
      "Eval Episode 1 Step 126: Action 3\n",
      "Eval Episode 1 Step 127: Action 3\n",
      "Eval Episode 1 Step 128: Action 3\n",
      "Eval Episode 1 Step 129: Action 1\n",
      "Eval Episode 1 Step 130: Action 1\n",
      "Eval Episode 1 Step 131: Action 1\n",
      "Eval Episode 1 Step 132: Action 2\n",
      "Eval Episode 1 Step 133: Action 1\n",
      "Eval Episode 1 Step 134: Action 3\n",
      "Eval Episode 1 Step 135: Action 3\n",
      "Eval Episode 1 Step 136: Action 3\n",
      "Eval Episode 1 Step 137: Action 3\n",
      "Eval Episode 1 Step 138: Action 3\n",
      "Eval Episode 1 Step 139: Action 1\n",
      "Eval Episode 1 Step 140: Action 1\n",
      "Eval Episode 1 Step 141: Action 1\n",
      "Eval Episode 1 Step 142: Action 1\n",
      "Eval Episode 1 Step 143: Action 1\n",
      "Eval Episode 1 Step 144: Action 1\n",
      "Eval Episode 1 Step 145: Action 0\n",
      "Eval Episode 1 Step 146: Action 3\n",
      "Eval Episode 1 Step 147: Action 1\n",
      "Eval Episode 1 Step 148: Action 1\n",
      "Eval Episode 1 Step 149: Action 1\n",
      "Eval Episode 1 Step 150: Action 1\n",
      "Eval Episode 1 Step 151: Action 0\n",
      "Eval Episode 1 Step 152: Action 3\n",
      "Eval Episode 1 Step 153: Action 3\n",
      "Eval Episode 1 Step 154: Action 2\n",
      "Eval Episode 1 Step 155: Action 2\n",
      "Eval Episode 1 Step 156: Action 2\n",
      "Eval Episode 1 Step 157: Action 3\n",
      "Eval Episode 1 Step 158: Action 3\n",
      "Eval Episode 1 Step 159: Action 0\n",
      "Eval Episode 1 Step 160: Action 3\n",
      "Eval Episode 1 Step 161: Action 0\n",
      "Eval Episode 1 Step 162: Action 0\n",
      "Eval Episode 1 Step 163: Action 0\n",
      "Eval Episode 1 Step 164: Action 0\n",
      "Eval Episode 1 Step 165: Action 0\n",
      "Eval Episode 1 Step 166: Action 0\n",
      "Eval Episode 1 Step 167: Action 0\n",
      "Eval Episode 1 Step 168: Action 1\n",
      "Eval Episode 1 Step 169: Action 1\n",
      "Eval Episode 1 Step 170: Action 1\n",
      "Eval Episode 1 Step 171: Action 0\n",
      "Eval Episode 1 Step 172: Action 3\n",
      "Eval Episode 1 Step 173: Action 1\n",
      "Eval Episode 1 Step 174: Action 1\n",
      "Eval Episode 1 Step 175: Action 1\n",
      "Eval Episode 1 Step 176: Action 1\n",
      "Eval Episode 1 Step 177: Action 1\n",
      "Eval Episode 1 Step 178: Action 2\n",
      "Eval Episode 1 Step 179: Action 2\n",
      "Eval Episode 1 Step 180: Action 2\n",
      "Eval Episode 1 Step 181: Action 3\n",
      "Eval Episode 1 Step 182: Action 3\n",
      "Eval Episode 1 Step 183: Action 2\n",
      "Eval Episode 1 Step 184: Action 0\n",
      "Eval Episode 1 Step 185: Action 1\n",
      "Eval Episode 1 Step 186: Action 1\n",
      "Eval Episode 1 Step 187: Action 1\n",
      "Eval Episode 1 Step 188: Action 1\n",
      "Eval Episode 1 Step 189: Action 1\n",
      "Eval Episode 1 Step 190: Action 1\n",
      "Eval Episode 1 Step 191: Action 1\n",
      "Eval Episode 1 Step 192: Action 1\n",
      "Eval Episode 1 Step 193: Action 1\n",
      "Eval Episode 1 Step 194: Action 1\n",
      "Eval Episode 1 Step 195: Action 1\n",
      "Eval Episode 1 Step 196: Action 1\n",
      "Eval Episode 1 Step 197: Action 1\n",
      "Eval Episode 1 Step 198: Action 1\n",
      "Eval Episode 1 Step 199: Action 1\n",
      "Eval Episode 1 Step 200: Action 1\n",
      "Eval Episode 1 Step 201: Action 3\n",
      "Eval Episode 1 Step 202: Action 2\n",
      "Eval Episode 1 Step 203: Action 0\n",
      "Eval Episode 1 Step 204: Action 0\n",
      "Eval Episode 1 Step 205: Action 0\n",
      "Eval Episode 1 Step 206: Action 3\n",
      "Eval Episode 1 Step 207: Action 0\n",
      "Eval Episode 1 Step 208: Action 0\n",
      "Eval Episode 1 Step 209: Action 1\n",
      "Eval Episode 1 Step 210: Action 0\n",
      "Eval Episode 1 Step 211: Action 0\n",
      "Eval Episode 1 Step 212: Action 3\n",
      "Eval Episode 1 Step 213: Action 3\n",
      "Eval Episode 1 Step 214: Action 3\n",
      "Eval Episode 1 Step 215: Action 3\n",
      "Eval Episode 1 Step 216: Action 3\n",
      "Eval Episode 1 Step 217: Action 3\n",
      "Eval Episode 1 Step 218: Action 1\n",
      "Eval Episode 1 Step 219: Action 1\n",
      "Eval Episode 1 Step 220: Action 1\n",
      "Eval Episode 1 Step 221: Action 1\n",
      "Eval Episode 1 Step 222: Action 1\n",
      "Eval Episode 1 Step 223: Action 1\n",
      "Eval Episode 1 Step 224: Action 1\n",
      "Eval Episode 1 Step 225: Action 1\n",
      "Eval Episode 1 Step 226: Action 1\n",
      "Eval Episode 1 Step 227: Action 0\n",
      "Eval Episode 1 Step 228: Action 0\n",
      "Eval Episode 1 Step 229: Action 3\n",
      "Eval Episode 1 Step 230: Action 3\n",
      "Eval Episode 1 Step 231: Action 2\n",
      "Eval Episode 1 Step 232: Action 3\n",
      "Eval Episode 1 Step 233: Action 1\n",
      "Eval Episode 1 Step 234: Action 1\n",
      "Eval Episode 1 Step 235: Action 1\n",
      "Eval Episode 1 Step 236: Action 1\n",
      "Eval Episode 1 Step 237: Action 1\n",
      "Eval Episode 1 Step 238: Action 1\n",
      "Eval Episode 1 Step 239: Action 1\n",
      "Eval Episode 1 Step 240: Action 3\n",
      "Eval Episode 1 Step 241: Action 3\n",
      "Eval Episode 1 Step 242: Action 1\n",
      "Eval Episode 1 Step 243: Action 3\n",
      "Eval Episode 1 Step 244: Action 3\n",
      "Eval Episode 1 Step 245: Action 1\n",
      "Eval Episode 1 Step 246: Action 1\n",
      "Eval Episode 1 Step 247: Action 1\n",
      "Eval Episode 1 Step 248: Action 1\n",
      "Eval Episode 1 Step 249: Action 1\n",
      "Eval Episode 1 Step 250: Action 1\n",
      "Eval Episode 1 Step 251: Action 1\n",
      "Eval Episode 1 Step 252: Action 1\n",
      "Eval Episode 1 Step 253: Action 0\n",
      "Eval Episode 1 Step 254: Action 1\n",
      "Eval Episode 1 Step 255: Action 3\n",
      "Eval Episode 1 Step 256: Action 1\n",
      "Eval Episode 1 Step 257: Action 1\n",
      "Eval Episode 1 Step 258: Action 1\n",
      "Eval Episode 1 Step 259: Action 1\n",
      "Eval Episode 1 Step 260: Action 3\n",
      "Eval Episode 1 Step 261: Action 3\n",
      "Eval Episode 1 Step 262: Action 1\n",
      "Eval Episode 1 Step 263: Action 3\n",
      "Eval Episode 1 Step 264: Action 3\n",
      "Eval Episode 1 Step 265: Action 3\n",
      "Eval Episode 1 Step 266: Action 1\n",
      "Eval Episode 1 Step 267: Action 3\n",
      "Eval Episode 1 Step 268: Action 1\n",
      "Eval Episode 1 Step 269: Action 1\n",
      "Eval Episode 1 Step 270: Action 1\n",
      "Eval Episode 1 Step 271: Action 1\n",
      "Eval Episode 1 Step 272: Action 1\n",
      "Eval Episode 1 Step 273: Action 1\n",
      "Eval Episode 1 Step 274: Action 1\n",
      "Eval Episode 1 Step 275: Action 1\n",
      "Eval Episode 1 Step 276: Action 1\n",
      "Eval Episode 1 Step 277: Action 1\n",
      "Eval Episode 1 Step 278: Action 1\n",
      "Eval Episode 1 Reward: 4.0\n",
      "\n",
      "Eval Episode 2 Step 1: Action 1\n",
      "Eval Episode 2 Step 2: Action 1\n",
      "Eval Episode 2 Step 3: Action 1\n",
      "Eval Episode 2 Step 4: Action 1\n",
      "Eval Episode 2 Step 5: Action 1\n",
      "Eval Episode 2 Step 6: Action 3\n",
      "Eval Episode 2 Step 7: Action 3\n",
      "Eval Episode 2 Step 8: Action 2\n",
      "Eval Episode 2 Step 9: Action 3\n",
      "Eval Episode 2 Step 10: Action 2\n",
      "Eval Episode 2 Step 11: Action 3\n",
      "Eval Episode 2 Step 12: Action 3\n",
      "Eval Episode 2 Step 13: Action 3\n",
      "Eval Episode 2 Step 14: Action 3\n",
      "Eval Episode 2 Step 15: Action 3\n",
      "Eval Episode 2 Step 16: Action 3\n",
      "Eval Episode 2 Step 17: Action 1\n",
      "Eval Episode 2 Step 18: Action 1\n",
      "Eval Episode 2 Step 19: Action 1\n",
      "Eval Episode 2 Step 20: Action 1\n",
      "Eval Episode 2 Step 21: Action 2\n",
      "Eval Episode 2 Step 22: Action 1\n",
      "Eval Episode 2 Step 23: Action 0\n",
      "Eval Episode 2 Step 24: Action 0\n",
      "Eval Episode 2 Step 25: Action 0\n",
      "Eval Episode 2 Step 26: Action 3\n",
      "Eval Episode 2 Step 27: Action 3\n",
      "Eval Episode 2 Step 28: Action 0\n",
      "Eval Episode 2 Step 29: Action 0\n",
      "Eval Episode 2 Step 30: Action 0\n",
      "Eval Episode 2 Step 31: Action 3\n",
      "Eval Episode 2 Step 32: Action 2\n",
      "Eval Episode 2 Step 33: Action 2\n",
      "Eval Episode 2 Step 34: Action 2\n",
      "Eval Episode 2 Step 35: Action 0\n",
      "Eval Episode 2 Step 36: Action 3\n",
      "Eval Episode 2 Step 37: Action 3\n",
      "Eval Episode 2 Step 38: Action 3\n",
      "Eval Episode 2 Step 39: Action 3\n",
      "Eval Episode 2 Step 40: Action 3\n",
      "Eval Episode 2 Step 41: Action 0\n",
      "Eval Episode 2 Step 42: Action 0\n",
      "Eval Episode 2 Step 43: Action 0\n",
      "Eval Episode 2 Step 44: Action 0\n",
      "Eval Episode 2 Step 45: Action 0\n",
      "Eval Episode 2 Step 46: Action 0\n",
      "Eval Episode 2 Step 47: Action 1\n",
      "Eval Episode 2 Step 48: Action 1\n",
      "Eval Episode 2 Step 49: Action 1\n",
      "Eval Episode 2 Step 50: Action 1\n",
      "Eval Episode 2 Step 51: Action 1\n",
      "Eval Episode 2 Step 52: Action 1\n",
      "Eval Episode 2 Step 53: Action 1\n",
      "Eval Episode 2 Step 54: Action 1\n",
      "Eval Episode 2 Step 55: Action 1\n",
      "Eval Episode 2 Step 56: Action 2\n",
      "Eval Episode 2 Step 57: Action 2\n",
      "Eval Episode 2 Step 58: Action 2\n",
      "Eval Episode 2 Step 59: Action 3\n",
      "Eval Episode 2 Step 60: Action 3\n",
      "Eval Episode 2 Step 61: Action 3\n",
      "Eval Episode 2 Step 62: Action 3\n",
      "Eval Episode 2 Step 63: Action 3\n",
      "Eval Episode 2 Step 64: Action 1\n",
      "Eval Episode 2 Step 65: Action 1\n",
      "Eval Episode 2 Step 66: Action 3\n",
      "Eval Episode 2 Step 67: Action 3\n",
      "Eval Episode 2 Step 68: Action 1\n",
      "Eval Episode 2 Step 69: Action 1\n",
      "Eval Episode 2 Step 70: Action 1\n",
      "Eval Episode 2 Step 71: Action 1\n",
      "Eval Episode 2 Step 72: Action 1\n",
      "Eval Episode 2 Step 73: Action 1\n",
      "Eval Episode 2 Step 74: Action 1\n",
      "Eval Episode 2 Step 75: Action 0\n",
      "Eval Episode 2 Step 76: Action 1\n",
      "Eval Episode 2 Step 77: Action 1\n",
      "Eval Episode 2 Step 78: Action 1\n",
      "Eval Episode 2 Step 79: Action 0\n",
      "Eval Episode 2 Step 80: Action 0\n",
      "Eval Episode 2 Step 81: Action 2\n",
      "Eval Episode 2 Step 82: Action 0\n",
      "Eval Episode 2 Step 83: Action 0\n",
      "Eval Episode 2 Step 84: Action 1\n",
      "Eval Episode 2 Step 85: Action 0\n",
      "Eval Episode 2 Step 86: Action 0\n",
      "Eval Episode 2 Step 87: Action 0\n",
      "Eval Episode 2 Step 88: Action 0\n",
      "Eval Episode 2 Step 89: Action 3\n",
      "Eval Episode 2 Step 90: Action 3\n",
      "Eval Episode 2 Step 91: Action 3\n",
      "Eval Episode 2 Step 92: Action 3\n",
      "Eval Episode 2 Step 93: Action 3\n",
      "Eval Episode 2 Step 94: Action 3\n",
      "Eval Episode 2 Step 95: Action 3\n",
      "Eval Episode 2 Step 96: Action 3\n",
      "Eval Episode 2 Step 97: Action 1\n",
      "Eval Episode 2 Step 98: Action 2\n",
      "Eval Episode 2 Step 99: Action 2\n",
      "Eval Episode 2 Step 100: Action 0\n",
      "Eval Episode 2 Step 101: Action 1\n",
      "Eval Episode 2 Step 102: Action 1\n",
      "Eval Episode 2 Step 103: Action 0\n",
      "Eval Episode 2 Step 104: Action 3\n",
      "Eval Episode 2 Step 105: Action 0\n",
      "Eval Episode 2 Step 106: Action 0\n",
      "Eval Episode 2 Step 107: Action 3\n",
      "Eval Episode 2 Step 108: Action 3\n",
      "Eval Episode 2 Step 109: Action 0\n",
      "Eval Episode 2 Step 110: Action 3\n",
      "Eval Episode 2 Step 111: Action 0\n",
      "Eval Episode 2 Step 112: Action 1\n",
      "Eval Episode 2 Step 113: Action 1\n",
      "Eval Episode 2 Step 114: Action 1\n",
      "Eval Episode 2 Step 115: Action 0\n",
      "Eval Episode 2 Step 116: Action 0\n",
      "Eval Episode 2 Step 117: Action 0\n",
      "Eval Episode 2 Step 118: Action 1\n",
      "Eval Episode 2 Step 119: Action 3\n",
      "Eval Episode 2 Step 120: Action 3\n",
      "Eval Episode 2 Step 121: Action 3\n",
      "Eval Episode 2 Step 122: Action 3\n",
      "Eval Episode 2 Step 123: Action 1\n",
      "Eval Episode 2 Step 124: Action 1\n",
      "Eval Episode 2 Step 125: Action 1\n",
      "Eval Episode 2 Step 126: Action 1\n",
      "Eval Episode 2 Step 127: Action 1\n",
      "Eval Episode 2 Step 128: Action 1\n",
      "Eval Episode 2 Step 129: Action 1\n",
      "Eval Episode 2 Step 130: Action 1\n",
      "Eval Episode 2 Step 131: Action 1\n",
      "Eval Episode 2 Step 132: Action 1\n",
      "Eval Episode 2 Step 133: Action 1\n",
      "Eval Episode 2 Step 134: Action 1\n",
      "Eval Episode 2 Step 135: Action 1\n",
      "Eval Episode 2 Step 136: Action 1\n",
      "Eval Episode 2 Step 137: Action 1\n",
      "Eval Episode 2 Step 138: Action 3\n",
      "Eval Episode 2 Step 139: Action 1\n",
      "Eval Episode 2 Step 140: Action 1\n",
      "Eval Episode 2 Step 141: Action 1\n",
      "Eval Episode 2 Step 142: Action 3\n",
      "Eval Episode 2 Step 143: Action 3\n",
      "Eval Episode 2 Step 144: Action 1\n",
      "Eval Episode 2 Step 145: Action 3\n",
      "Eval Episode 2 Step 146: Action 1\n",
      "Eval Episode 2 Step 147: Action 1\n",
      "Eval Episode 2 Step 148: Action 1\n",
      "Eval Episode 2 Step 149: Action 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evalEnv = gym.make(EnvName, render_mode=\"human\")\n",
    "agent.policy.frameCount = EpsilonDecayFrames\n",
    "\n",
    "numEvalEpisodes = 5\n",
    "print(\"\\n=== Final Evaluation with Visualization ===\")\n",
    "\n",
    "for evalEpisode in range(1, numEvalEpisodes + 1):\n",
    "    obs, info = evalEnv.reset()\n",
    "    frameDeque.clear()\n",
    "    state = StackFrames(frameDeque, obs, True)\n",
    "    episodeReward = 0.0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = agent.policy.SelectAction(agent.onlineModel, state)\n",
    "        obs, reward, terminated, truncated, info = evalEnv.step(action)\n",
    "        done = terminated or truncated\n",
    "        nextState = StackFrames(frameDeque, obs, False)\n",
    "        episodeReward += reward\n",
    "        state = nextState\n",
    "\n",
    "        print(f\"Eval Episode {evalEpisode} Step {step}: Action {action}\")\n",
    "\n",
    "        time.sleep(0.02)  # adjust speed, 20ms delay for smoothness\n",
    "\n",
    "    print(f\"Eval Episode {evalEpisode} Reward: {episodeReward}\\n\")\n",
    "\n",
    "evalEnv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
